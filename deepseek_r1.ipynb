{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain-core Langgraph  Langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama 모델 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model = \"deepseek-r1:7b\", temperature = 0.5)\n",
    "exaone = ChatOllama(model = \"exaone3.5:7.8b\", temperature = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek 3.9, 3.11 대소 비교 (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "To determine which number is larger between 9.9 and 9.11, I will first ensure that both numbers have the same number of decimal places.\n",
      "\n",
      "I can rewrite 9.9 as 9.90 to match the two decimal places of 9.11.\n",
      "\n",
      "Next, I'll compare each corresponding digit starting from the units place.\n",
      "\n",
      "Both numbers have a 9 in the units place, so they are equal there.\n",
      "\n",
      "Moving to the tenths place, 9.90 has a 9 while 9.11 has a 1. Since 9 is greater than 1, 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Align the Decimal Places\n",
      "First, ensure both numbers have the same number of decimal places for an accurate comparison.\n",
      "\n",
      "- **9.9** can be written as **9.90** (adding a zero at the end doesn't change its value).\n",
      "- **9.11** remains as is.\n",
      "\n",
      "Now, we compare:\n",
      "- **9.90**\n",
      "- **9.11**\n",
      "\n",
      "### Step 2: Compare Digit by Digit\n",
      "Starting from the left (units place), compare each corresponding digit:\n",
      "\n",
      "1. **Units Place:** Both numbers have a **9**.\n",
      "   \n",
      "2. **Tenths Place:** \n",
      "   - **9.90** has a **9** in the tenths place.\n",
      "   - **9.11** has a **1** in the tenths place.\n",
      "   \n",
      "Since **9 > 1**, **9.90** is greater than **9.11**.\n",
      "\n",
      "### Conclusion\n",
      "Therefore, **9.9** (or **9.90**) is larger than **9.11**.\n",
      "\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is bigger than } 9.11}\n",
      "\\]"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "To determine which number is larger between 9.9 and 9.11, I will first ensure that both numbers have the same number of decimal places.\n",
       "\n",
       "I can rewrite 9.9 as 9.90 to match the two decimal places of 9.11.\n",
       "\n",
       "Next, I'll compare each corresponding digit starting from the units place.\n",
       "\n",
       "Both numbers have a 9 in the units place, so they are equal there.\n",
       "\n",
       "Moving to the tenths place, 9.90 has a 9 while 9.11 has a 1. Since 9 is greater than 1, 9.90 is larger than 9.11.\n",
       "\n",
       "Therefore, 9.9 is bigger than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step.\n",
       "\n",
       "### Step 1: Align the Decimal Places\n",
       "First, ensure both numbers have the same number of decimal places for an accurate comparison.\n",
       "\n",
       "- **9.9** can be written as **9.90** (adding a zero at the end doesn't change its value).\n",
       "- **9.11** remains as is.\n",
       "\n",
       "Now, we compare:\n",
       "- **9.90**\n",
       "- **9.11**\n",
       "\n",
       "### Step 2: Compare Digit by Digit\n",
       "Starting from the left (units place), compare each corresponding digit:\n",
       "\n",
       "1. **Units Place:** Both numbers have a **9**.\n",
       "   \n",
       "2. **Tenths Place:** \n",
       "   - **9.90** has a **9** in the tenths place.\n",
       "   - **9.11** has a **1** in the tenths place.\n",
       "   \n",
       "Since **9 > 1**, **9.90** is greater than **9.11**.\n",
       "\n",
       "### Conclusion\n",
       "Therefore, **9.9** (or **9.90**) is larger than **9.11**.\n",
       "\n",
       "\\[\n",
       "\\boxed{9.9 \\text{ is bigger than } 9.11}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer = []\n",
    "for chunk in deepseek.stream(\"What is bigger than between 9.9 and 9.11\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end = \"\", flush = True) # 대화형처럼 출력되기 위해 flush = True\n",
    "\n",
    "answer_md = ''.join([i for i in answer])\n",
    "display(Markdown(answer_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존 LLM이 답변을 잘 못하는 문제에대해서 추론모델이여서 답변을 잘 하는 것을 확인, 그리고 확실히 영어 질문에 대해서 답변을 잘함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek 3.9, 3.11 대소 비교 (Korean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "먼저, 9.9와 9.11을 비교하는 기준을 결정합니다.  decimla point 를 based 로 비교하는 가장 atural method 입니다. \n",
      "\n",
      "다른 방법으로 convert dozen decimal numbers integer form 에 가 수 can compare easier manner to see difference clearly .\n",
      "\n",
      "9.9 를 99/10 로, 9.11 을 911/100 로 나타내어 비교합니다. \n",
      "\n",
      "이cstdlib commom denominator 를 찾는  thing is 필요하고,  common denominator 100 를 사용하여 9.9 를 990/100 로 나타내 manner can compare two numbers equally .\n",
      "\n",
      "이제 990/100 와 911/100 을 비교합니다. \n",
      "\n",
      "990 은 911 보다 다른 greater 기 때문에, 9.9 는 9.11 보다 큰 数值 입니다.\n",
      "</think>\n",
      "\n",
      "**문제:** 9.9와 9.11 중 무엇이 더 크니?\n",
      "\n",
      "**解答:**\n",
      "\n",
      "9.9와 9.11을 비교할 때, 다른 방법으로 decimals 를 integer form 로 바꾸어 비교하는 것이 가reatest easy method 입니다.\n",
      "\n",
      "1. **Convert to Integer Form:**\n",
      "   - 9.9 = \\( \\frac{99}{10} = 990/100 \\)\n",
      "   - 9.11 = \\( \\frac{911}{100} \\)\n",
      "\n",
      "2. **Comparison:**\n",
      "   - 990/100 는 911/100 보다 다른 greater 기 때문에, 9.9 는 9.11 보다 큰 数值 입니다.\n",
      "\n",
      "**결론:**\n",
      "\n",
      "\\[\n",
      "\\boxed{9.9}\n",
      "\\]"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "먼저, 9.9와 9.11을 비교하는 기준을 결정합니다.  decimla point 를 based 로 비교하는 가장 atural method 입니다. \n",
       "\n",
       "다른 방법으로 convert dozen decimal numbers integer form 에 가 수 can compare easier manner to see difference clearly .\n",
       "\n",
       "9.9 를 99/10 로, 9.11 을 911/100 로 나타내어 비교합니다. \n",
       "\n",
       "이cstdlib commom denominator 를 찾는  thing is 필요하고,  common denominator 100 를 사용하여 9.9 를 990/100 로 나타내 manner can compare two numbers equally .\n",
       "\n",
       "이제 990/100 와 911/100 을 비교합니다. \n",
       "\n",
       "990 은 911 보다 다른 greater 기 때문에, 9.9 는 9.11 보다 큰 数值 입니다.\n",
       "</think>\n",
       "\n",
       "**문제:** 9.9와 9.11 중 무엇이 더 크니?\n",
       "\n",
       "**解答:**\n",
       "\n",
       "9.9와 9.11을 비교할 때, 다른 방법으로 decimals 를 integer form 로 바꾸어 비교하는 것이 가reatest easy method 입니다.\n",
       "\n",
       "1. **Convert to Integer Form:**\n",
       "   - 9.9 = \\( \\frac{99}{10} = 990/100 \\)\n",
       "   - 9.11 = \\( \\frac{911}{100} \\)\n",
       "\n",
       "2. **Comparison:**\n",
       "   - 990/100 는 911/100 보다 다른 greater 기 때문에, 9.9 는 9.11 보다 큰 数值 입니다.\n",
       "\n",
       "**결론:**\n",
       "\n",
       "\\[\n",
       "\\boxed{9.9}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer = []\n",
    "for chunk in deepseek.stream(\"9.9와 9.11 중 무엇이 더 크니?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end = \"\", flush = True) # 대화형처럼 출력되기 위해 flush = True\n",
    "    \n",
    "answer_md = ''.join([i for i in answer])\n",
    "display(Markdown(answer_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "언어가 r1-zero에서 보듯 다국어가 혼용되서 출력\\\n",
    "이유 추측 : 모델 파라미터가 작음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.11이 9.9보다 큽니다. 숫자를 비교할 때는 소수점을 무시하고 정수 부분만 보아도 쉽게 알 수 있습니다. 여기서는 9.11이 더 큰 값입니다."
     ]
    }
   ],
   "source": [
    "## 한글 로컬 모델로 풀어보기 - Exaone(추론 x)\n",
    "answer = []\n",
    "for chunk in exaone.stream(\"9.9와 9.11 중 무엇이 더 크니?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end = \"\", flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "답변이 틀린것을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지 알 수 있는 점\n",
    "- Deepseek r1은 추론은 잘 하여 일반 LLM 대비 높은 성능을 보여주지만 한국어 생성 능력이 부족함\n",
    "- Exaone은 한국어 생성 능력이 뛰어나지만 추론 능력이 부족함\n",
    "\n",
    "이 두개를 결합하여 한국어 생성 능력과 추론 능력을 모두 갖춘 모델을 만들어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek(추론) + Exaone(답변)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "reasoning_model = ChatOllama(model = \"deepseek-r1:7b\",\n",
    "                       temperature = 0,\n",
    "                       stop = [\"</think>\"]) # </think> 토큰이 나올때까지 토큰 생성 -> 추론모델이 think 태그까지 추론하는 부분을 살림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model = ChatOllama(model = \"exaone3.5:7.8b\",\n",
    "                              temperature = 0.7\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Langgraph에서 state는 노드 간의 정보를 전달할 틀\n",
    "# 노드 간에 계속 전달하고 싶거나, 그래프 내에서 유지해야할 정보를 미리 정의\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "# gpt-o3로 prompt 생성    \n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "You are a helpful AI assistant that provides clear and comprehensive answers based on given reasoning.\n",
    "\n",
    "Your task is to:\n",
    "- Carefully analyze both the question and the provided reasoning\n",
    "- Generate a well-structured response that incorporates the insights from the reasoning\n",
    "- Ensure your answer directly addresses the user's question\n",
    "- Present the information in a clear and natural way without explicitly mentioning the reasoning process\n",
    "\n",
    "Guidelines:\n",
    "- Make your response conversational and engaging\n",
    "- Focus on being clear and concise while covering all important points\n",
    "- Don't mention that you're using provided reasoning - just incorporate those insights naturally\n",
    "- Maintain a helpful and professional tone\n",
    "\n",
    "Remember: Your goal is to provide a natural, informative response that seamlessly incorporates\n",
    "the insights from the reasoning process while directly addressing the user's question.\n",
    "    \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        Question : {question}\n",
    "        Thinking : {thinking}\n",
    "        \"\"\"\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Deepseek를 통해서 추론부분까지 생성 - think node\n",
    "def think(state : State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question) # r1 모델을 통해 추론부분까지 생성\n",
    "    return {\"thinking\" : response.content}\n",
    "\n",
    "# Exaone을 통해서 답변 생성\n",
    "def generate(state : State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    return {\"answer\" : response.content}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "먼저, 9.9와 9.11을 비교해야 합니다.\n",
      "\n",
      "9.9는 한 자리 หลoss  paise 로 0.1  unit 을 나타내고  있습니다.\n",
      "9.11은 이  paise 를 다 나타내고  0.01  unit 을 나타내고  있습니다.\n",
      "\n",
      "9.9 는 9.90  unit 로 同等  9  unit 와 90  paise 를 나타내고  있습니다.\n",
      "9.11 는 9  unit 와 11  paise 를 나타내고  있습니다.\n",
      "\n",
      "90  paise 는 10  paise 보다  8  paise 가  적 Fewer  than 11  paise 이 기ecause 1  unit 은 100  paise 입니다.\n",
      "9.11이 9.9보다 더 큽니다. \n",
      "\n",
      "숫자를 쉽게 비교해보면, 둘 다 '9'로 시작하지만 9.11은 소수점 아래에 '11'이라는 숫자가 더 큽니다. 이는 11 paise가 9 paise보다 더 많다는 것을 의미합니다. 결국, 9.11은 9.9보다 약간 더 큰 값을 나타냅니다."
     ]
    }
   ],
   "source": [
    "inputs = {\"question\" : \"9.9와 9.11 중 무엇이 더 큰가요??\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version = \"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end = \"\", flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아직도 다국어가 혼영되는걸로 보여 프롬프트안에서 바꾸지는 못할거 같고 모델의 한계인것으로보여 다국어로 학습된 r1을 가져와서 동일 코드 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "reasoning_model = ChatOllama(model = \"kwangsuklee/DeepSeek-R1-Distill-Qwen-7B-Multilingual-Q5_K_M:latest\", # 다국어 finetuning 모델\n",
    "                       temperature = 0,\n",
    "                       stop = [\"</think>\"]) # </think> 토큰이 나올때까지 토큰 생성 -> 추론모델이 think 태그까지 추론하는 부분을 살림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Langgraph에서 state는 노드 간의 정보를 전달할 틀\n",
    "# 노드 간에 계속 전달하고 싶거나, 그래프 내에서 유지해야할 정보를 미리 정의\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "# gpt-o3로 prompt 생성    \n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "You are a helpful AI assistant that provides clear and comprehensive answers based on given reasoning.\n",
    "\n",
    "Your task is to:\n",
    "- Carefully analyze both the question and the provided reasoning\n",
    "- Generate a well-structured response that incorporates the insights from the reasoning\n",
    "- Ensure your answer directly addresses the user's question\n",
    "- Present the information in a clear and natural way without explicitly mentioning the reasoning process\n",
    "\n",
    "Guidelines:\n",
    "- Make your response conversational and engaging\n",
    "- Focus on being clear and concise while covering all important points\n",
    "- Don't mention that you're using provided reasoning - just incorporate those insights naturally\n",
    "- Maintain a helpful and professional tone\n",
    "\n",
    "Remember: Your goal is to provide a natural, informative response that seamlessly incorporates\n",
    "the insights from the reasoning process while directly addressing the user's question.\n",
    "    \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        Question : {question}\n",
    "        Thinking : {thinking}\n",
    "        \"\"\"\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Deepseek를 통해서 추론부분까지 생성 - think node\n",
    "def think(state : State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question) # r1 모델을 통해 추론부분까지 생성\n",
    "    return {\"thinking\" : response.content}\n",
    "\n",
    "# Exaone을 통해서 답변 생성\n",
    "def generate(state : State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    return {\"answer\" : response.content}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "사용자가 9.9와 9.11 중 무엇이 더 큰지 물어보았습니다. 이는 두 수의 비교를 통해 정확한 답변을 제공해야 합니다.\n",
      "\n",
      "먼저, 9.9와 9.11을 비교하기 위해 소수점 이하의 자리수를 보겠습니다. 9.9는 9.90로, 9.11은 9.11로, 소수점 이하 두 자리 수로 표현할 수 있습니다.\n",
      "\n",
      "다음으로, 소수점 이하의 자리수를 비교합니다. 9.90의 소수점 이하 두 자리 수는 90, còn 9.11은 11입니다. 90이 11보다 크기 때문에, 9.90이 9.11보다 크다고 말할 수 있습니다.\n",
      "\n",
      "또한, 9.9와 9.11을 소수점 이하로만 비교하는 것이 중요합니다. 9.9는 9.90으로, 9.11은 9.11로, 9.90이 9.11보다 크기 때문에, 9.9가 9.11보다 큰 수입니다.\n",
      "\n",
      "마지막으로, 사용자가 9.9와 9.11 중 무엇이 더 큰지 물어보았기 때문에, 정확한 답변을 제공하기 위해 이 두 수의 비교를 통해 9.9가 9.11보다 큰 수라는 결론을 내릴 수 있습니다.\n",
      "사용자님, 9.9와 9.11 중에서 더 큰 수는 **9.9**입니다. \n",
      "\n",
      "두 숫자를 비교할 때, 소수점 아래의 자릿수까지 살펴보면 9.9는 실제로 9.90과 동일하게 볼 수 있습니다. 그리고 90이 11보다 크므로, 9.9가 9.11보다 더 큰 수임을 알 수 있습니다. 따라서 쉽게 말해, 숫자 줄에서 9.9가 9.11보다 오른쪽에 위치해 더 큰 값을 나타냅니다."
     ]
    }
   ],
   "source": [
    "inputs = {\"question\" : \"9.9와 9.11 중 무엇이 더 큰가요??\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version = \"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end = \"\", flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조금 섞여 있는게 보이지만 그래도 전보다 나은 결과를 보여준다, 그리고 애초에 다국어 모델이니까 굳이 reasoning, generate model을 구분 할 필요도 없을거같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 밑으로는 LLM을 RAG에 결합시켜서 PDF탐색까지 한 번 해보자 - 이건 r1 multilingual만 활용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
